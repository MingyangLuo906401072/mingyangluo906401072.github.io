---
title: "Span Wines Anaysis"
author: "Mingyang Luo"
date: "2023-11-08"
categories: [Probability theory, Classification, Anomaly Detection]
image: "image.jpg"
---

$\textbf{Naive Bayes}$

The Naive Bayes methods are a collection of supervised learning algorithms that utilize Bayes' theorem with a "naive" assumption of conditional independence among all pairs of features when considering the value of the class variable. Bayes' theorem expresses the following relationship, considering a class variable $\textit{y}$ and a dependent feature vector $x_1$ through $x_n$:

$P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots, x_n \mid y)}{P(x_1, \dots, x_n)}$

This relationship allows for the calculation of the probability of a class variable given the observed features, under the assumption that the features are conditionally independent given the class variable.

Using the naive conditional independence assumption that:

$P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y)$,

for all $i$, this relationship is simplified to:

$P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)} {P(x_1, \dots, x_n)}$

Since \$P(x_1, \\dots, x_n)\$ is constant given the input, we can use the following classification rule:

$\begin{align}\begin{aligned}P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)\\\Downarrow\\\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),\end{aligned}\end{align}$

then we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \mid y)$; the former is then the relative frequency of class $y$ in the training set.

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, recall_score, classification_report
from sklearn.model_selection import learning_curve
from sklearn.metrics import confusion_matrix
from sklearn.svm import OneClassSVM
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

csv_file_path = "wines_SPA.csv"
df = pd.read_csv(csv_file_path)
df
```

```{python}
unique_country = df['country'].nunique()
print(f"Number of unique country: {unique_country}")
```

Build a Gaussian Naive Bayes model adeptly predicts wine types leveraging the provided features. The classification report outlines the model's precision and recall for each wine class, underscoring its reliability in discerning between different wine categories. This highlights the model's potential in the realm of wine classification with high overall accuracy.

```{python}
categorical_columns = ['winery', 'region', 'type']
numeric_columns = ['year', 'rating', 'num_reviews', 'price', 'body', 'acidity']

X_categorical = df[categorical_columns]
X_numeric = df[numeric_columns]
X_numeric['year'] = pd.to_numeric(X_numeric['year'], errors='coerce')

nan_indices = df['wine'].index[df['wine'].isnull()]
X_categorical = X_categorical.drop(nan_indices)
X_numeric = X_numeric.drop(nan_indices)
y = df['wine'].drop(nan_indices)

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),  
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=2))
])

X_processed_numeric = numeric_transformer.fit_transform(X_numeric)

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
])

X_processed_categorical = categorical_transformer.fit_transform(X_categorical)

result_df_numeric = pd.DataFrame(data=X_processed_numeric, columns=['PC1', 'PC2'])
result_df_categorical = pd.DataFrame(X_processed_categorical, columns=categorical_transformer.named_steps['onehot'].get_feature_names_out(categorical_columns))
result_df = pd.concat([result_df_numeric, result_df_categorical], axis=1)

X_train, X_test, y_train, y_test = train_test_split(result_df, y, test_size=0.2, random_state=42)

nb_model = GaussianNB(var_smoothing=1e-02)
nb_model.fit(X_train, y_train)

predictions = nb_model.predict(X_test)

accuracy = accuracy_score(y_test, predictions)
recall = recall_score(y_test, predictions, average='weighted')  # Change average as needed
report = classification_report(y_test, predictions)

print(f"Accuracy: {accuracy:.2f}")
print(f"Recall: {recall:.2f}")
print("Classification Report:")
print(report)
```

Below calculates the maximum probability for each sample in the test dataset generating a kernel density plot (KDE) to visualize the distribution of these maximum predicted probabilities.

For each data point, a kernel function (often a Gaussian kernel) is placed at that point, and these kernels are summed up to create the KDE. The width of the kernel determines how much influence each data point has on the estimation at different points along the x-axis.

The formula for kernel density estimation involves the following steps:

For each data point x, a kernel function K is placed at that point.

The kernel function is often a Gaussian function centered at x.

These kernel functions are summed (or averaged) to get the density estimate.

The general formula for a Gaussian kernel is:

$K(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} u^2}$

Where u is a standardized distance ($\frac{x - \mu}{\sigma}$)

The density values in a KDE plot represent the relative likelihood or concentration of the data points at different values along the x-axis. The y-axis values in a KDE plot show the density of occurrences, and the area under the curve represents the probability of the data falling within a specific range.

```{python}
probabilities = nb_model.predict_proba(X_test)
max_probabilities = np.max(probabilities, axis=1)

plt.figure(figsize=(8, 6))
sns.kdeplot(max_probabilities, shade=True, label='Max Probability Density')
plt.title('Max Probability Density Plot')
plt.xlabel('Maximum Predicted Probability')
plt.ylabel('Density')
plt.legend()
plt.show()
```

```{python}
print(predictions)
```

Plot a sparse confusion matrix

```{python}
conf_matrix = confusion_matrix(y_test, predictions)
conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(14, 10))
sns.heatmap(conf_matrix_norm, annot=False, cmap='Purples')  
plt.title('Normalized Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
# YlGnBu
```

```{python}
pca_model = numeric_transformer.named_steps['pca']
explained_variance = pca_model.explained_variance_ratio_

plt.figure(figsize=(8, 6))
plt.scatter(result_df['PC1'], result_df['PC2'], c='coral', alpha=0.5)
plt.title('PCA Components: PC1 vs PC2')
plt.xlabel('Principal Component 1 (PC1)')
plt.ylabel('Principal Component 2 (PC2)')

# Annotate explained variance
plt.annotate(f'Explained Variance PC1: {explained_variance[0]:.2f}',
             xy=(result_df['PC1'].min(), result_df['PC2'].max()), 
             xytext=(result_df['PC1'].min() + 1, result_df['PC2'].max() - 1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.annotate(f'Explained Variance PC2: {explained_variance[1]:.2f}',
             xy=(result_df['PC1'].max(), result_df['PC2'].min()), 
             xytext=(result_df['PC1'].max() - 2, result_df['PC2'].min() + 1),
             arrowprops=dict(facecolor='black', arrowstyle='->'))

plt.grid(True)
plt.show()
```

One-Class SVM for anomaly detection identifiying potential outliers within the dataset. Leveraging the algorithm with a parameter setting of nu=0.03, anomalies are detected and visualized against the principal components in a scatter plot. The distinct identification of anomalies marked in red ('Anomaly (SVM)') amid the 'Normal' data points in coral underscores the efficacy of the One-Class SVM in pinpointing potential outliers within the dataset. This visualization aids in understanding and delineating data points that deviate significantly from the norm, preparing for further investigation or outlier handling in the dataset.

```{python}
svm = OneClassSVM(nu=0.03) 
outliers_svm = svm.fit_predict(X_processed_numeric)
result_df['is_outlier_svm'] = outliers_svm

anomaly_points_svm = result_df[result_df['is_outlier_svm'] == -1]

plt.figure(figsize=(8, 6))
plt.scatter(result_df['PC1'], result_df['PC2'], alpha=0.5, c='coral', label='Normal')
plt.scatter(anomaly_points_svm['PC1'], anomaly_points_svm['PC2'], color='red', marker='x', label='Anomaly (SVM)')
plt.title('PCA Components: PC1 vs PC2 with Anomalies (SVM)')
plt.xlabel('Principal Component 1 (PC1)')
plt.ylabel('Principal Component 2 (PC2)')
plt.legend()
plt.grid(True)
plt.show()
```
