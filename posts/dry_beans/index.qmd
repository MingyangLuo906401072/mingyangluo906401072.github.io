---
title: "Dry Beans Anaysis"
author: "Mingyang Luo"
date: "2023-11-08"
categories: [Classification, Clustering, Anomaly Detection]
image: "db.jpg"
---

**`DecisionTreeClassifier`** operates by partitioning the dataset into subsets based on the values of input features. It creates a tree-like structure where each node represents a feature, each branch denotes a decision based on that feature, and each leaf node signifies the class label or outcome. While decision trees are interpretable and intuitive, they are prone to overfitting when the tree grows too complex.

On the other hand, **`RandomForestClassifier`** is an ensemble learning method that builds multiple decision trees and amalgamates their predictions to make a more robust and accurate classification. It operates by constructing a multitude of decision trees, each trained on a random subset of the dataset and using a random subset of features. The final prediction is determined by aggregating the predictions of all the individual trees, usually through voting or averaging. This ensemble approach tends to reduce overfitting and enhances the model's generalizability.

The differences lie in their methodologies and performances. Decision trees are simple to interpret but might overfit the training data. Random forests, leveraging the wisdom of multiple trees, tend to offer higher accuracy and better resilience against overfitting. However, they might be less interpretable due to the complexity of aggregating predictions from multiple trees.

```{python}
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.model_selection import learning_curve
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, roc_auc_score
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import scipy.cluster.hierarchy as sch
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import recall_score
from sklearn.svm import SVC
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import label_binarize
import warnings

warnings.filterwarnings("ignore")

excel_file_path = "DryBeanDataset/Dry_Bean_Dataset.xlsx"
df = pd.read_excel(excel_file_path)
df
```

Parallel coordinate plots, a visualization method, display how different categories behave across various standardized numeric dimensions, offering a clear visual narrative of patterns and trends within the dataset. Together, these tools provide a powerful lens to explore and understand relationships among normalized numeric features.

```{python}
numeric_df = df.select_dtypes(include=['number'])
cols = numeric_df.columns
scaler = StandardScaler()
numeric_df = scaler.fit_transform(numeric_df)
pca = PCA(n_components=2)
df_pca = pca.fit_transform(numeric_df)
normalized_df = pd.DataFrame(numeric_df, columns=cols)

plt.figure(figsize=(23, 6))  
pd.plotting.parallel_coordinates(normalized_df.join(df.Class), class_column='Class', colormap='hsv') 
plt.title('Standardized Parallel Coordinates Plot')
plt.ylabel('Values')
plt.show()
```

Use KMeans clustering for visualization of how the dry bean dataset organizes into distinct clusters. The identified anomalies, highlighted in the plot, serve as potential outliers worth examining further. This methodology aids in both understanding the dataset's natural groupings and pinpointing potential irregularities.

```{python}
kmeans = KMeans(n_clusters=7, random_state=42)
df['Cluster'] = kmeans.fit_predict(df_pca)
labels = kmeans.labels_

cluster_centers = kmeans.cluster_centers_

distances = []
for i, label in enumerate(kmeans.labels_):
    center = cluster_centers[label]
    point = df_pca[i]
    distance = np.linalg.norm(point - center)  # Euclidean distance
    distances.append(distance)

threshold = np.percentile(distances, 99)  
anomalies = df_pca[np.array(distances) > threshold]

plt.figure(figsize=(8, 6))
plt.scatter(df_pca[:, 0], df_pca[:, 1], c=labels, cmap='viridis', marker='o', alpha=0.6)
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], s=100, c='cyan', marker='o', label='Cluster Centers')
plt.xlabel(f'Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.3f}')
plt.ylabel(f'Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.3f}')
plt.title('Clustering of Dry Beans using KMeans with 7 clusters')
plt.scatter(anomalies[:, 0], anomalies[:, 1], c='red', marker='x', s=100, label='Anomalies')
plt.legend()
plt.show()
```

Visualize a Decision Tree Classifier's decision boundary in a PCA-transformed dataset. By creating a meshgrid and predicting class labels across this grid, the plot illustrates how the classifier categorizes data in the transformed feature space.

```{python}
X1 = df_pca
y1 = df['Class']
label_encoder = LabelEncoder()
y1 = label_encoder.fit_transform(y1)
feature_1, feature_2 = np.meshgrid(
    np.linspace(X1[:, 0].min(), X1[:, 0].max()),
    np.linspace(X1[:, 1].min(), X1[:, 1].max())
)
grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T
tree = DecisionTreeClassifier().fit(X1, y1)
y_pred1 = tree.predict(grid)
y_pred1 = y_pred1.reshape(feature_1.shape)
display = DecisionBoundaryDisplay(xx0=feature_1, xx1=feature_2, response=y_pred1)
display.plot()
plt.scatter(X1[:, 0], X1[:, 1], c=y1, edgecolor="k", s=10)
plt.show()
```

Generates a summarized view of the distribution of classes within each cluster. The resulting table showcases the count of each class within every cluster, describing how classes are distributed across different clusters in the dataset.

```{python}
cluster_data = pd.DataFrame({'Cluster': df['Cluster'], 'Class': df['Class']})  
cluster_summary = cluster_data.groupby('Cluster')['Class'].value_counts().unstack().fillna(0)
print(cluster_summary)
```

Generates a confusion matrix (cm) comparing predicted labels y_pred against the actual labels y_test to evaluate model performance. The ConfusionMatrixDisplay from scikit-learn visualizes this matrix, providing insights into the classifier's accuracy in predicting different classes. The color-coded matrix, displayed using a blue colormap, demonstrates the classifier's performance across different classes, enabling an assessment of its predictive strengths and weaknesses.

```{python}
X = numeric_df 
y = df['Class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier(max_depth=8, random_state=42)
clf.fit(X_train, y_train)

y_scores = clf.predict_proba(X_test)
y_pred = clf.predict(X_test)

cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(cmap='Blues', values_format='d')
plt.show()
```

The resulting plot visualizes the ROC curves for each class, with the x-axis representing the False Positive Rate and the y-axis representing the True Positive Rate. The dashed diagonal line represents a random classifier. The curves' proximity to the upper-left corner indicates better classification performance, and the AUC values quantify the overall predictive power for each class.

```{python}
y_bin = label_binarize(y_test, classes=clf.classes_)

fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(len(clf.classes_)):
    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(8, 6))

for i in range(len(clf.classes_)):
    plt.plot(fpr[i], tpr[i], label=f'ROC curve (area = {roc_auc[i]:.2f}) for class {clf.classes_[i]}')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass Classification')
plt.legend(loc="lower right")
plt.show()
```

The shaded areas around each curve depict the variance (standard deviation) of the scores across different cross-validation folds for both training and testing data. This learning curve explains the model's behavior related to dataset sizes.

```{python}
train_sizes, train_scores, test_scores = learning_curve(clf, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring='accuracy')

train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

plt.figure()
plt.title("Learning Curve")
plt.xlabel("Training examples")
plt.ylabel("Accuracy")

plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g")

plt.legend()
plt.show()
```

This visualization serves as a visual roadmap of the Decision Tree Classifier's decision-making process. It encapsulates how the classifier discerns between different classes based on distinct features. The parameter max_depth=4 limits the depth of the tree, enhancing its readability and interpretability.

An interesting observation emerges from this visualization: the 'BOMBAY' class is entirely classified within the 4th depth of this decision tree. This finding aligns seamlessly with the ROC curve analysis, indicating a perfect 100 percent true positive rate for 'BOMBAY'. This convergence between the decision tree's structure and the ROC curve reinforces the accuracy of the classifier in precisely identifying instances belonging to the 'BOMBAY' class.

The correlation showcases model's capability to accurately classify instances of the 'BOMBAY' class, which could well explain previous and subsequent parts' roc curve's behavior.

```{python}
from IPython.display import Image
import pydot
from sklearn.tree import export_graphviz
import graphviz

data=export_graphviz(
    clf,
    feature_names=df.drop(columns=['Class', 'Cluster']).columns,
    class_names=clf.classes_,
    rounded=True,
    filled=True,
    max_depth=4,
)

graph = pydot.graph_from_dot_data(data)
Image(graph[0].create_png())

# graph = graphviz.Source(data)
# graph.render("decision_tree")
```

```{python}
roc_auc = roc_auc_score(y_test, y_scores, multi_class='ovo')  
print(f'ROC AUC Score: {roc_auc:.2f}')
```

```{python}
scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
mean_score = scores.mean()
std_score = scores.std()
print(f'Standard Error Score: {std_score:.2f}')
```

```{python}
# recall = recall_score(y_test, y_pred, average=None)
recall = recall_score(y_test, y_pred, average='macro')
print(f'Recall Score: {recall:.2f}')
```

```{python}
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)

clf1 = RandomForestClassifier(n_estimators=20, max_depth=8, random_state=42)
clf1.fit(X_train1, y_train1)

y_scores1 = clf1.predict_proba(X_test1)

fpr1 = dict()
tpr1 = dict()
roc_auc1 = dict()

for i in range(len(clf1.classes_)):
    fpr1[i], tpr1[i], _ = roc_curve((y_test1 == clf1.classes_[i]).astype(int), y_scores1[:, i])
    roc_auc1[i] = auc(fpr1[i], tpr1[i])

plt.figure(figsize=(8, 6))

for i in range(len(clf1.classes_)):
    plt.plot(fpr1[i], tpr1[i], label=f'ROC curve (area = {roc_auc1[i]:.2f}) for class {clf1.classes_[i]}')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass Classification')
plt.legend(loc="lower right")
plt.show()
```

```{python}
train_sizes1, train_scores1, test_scores1 = learning_curve(clf1, X_train1, y_train1, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring='accuracy')

train_scores_mean1 = np.mean(train_scores1, axis=1)
train_scores_std1 = np.std(train_scores1, axis=1)
test_scores_mean1 = np.mean(test_scores1, axis=1)
test_scores_std1 = np.std(test_scores1, axis=1)

plt.figure()
plt.title("Learning Curve")
plt.xlabel("Training examples")
plt.ylabel("Accuracy")

plt.plot(train_sizes1, train_scores_mean1, 'o-', color="r", label="Training score")
plt.plot(train_sizes1, test_scores_mean1, 'o-', color="g", label="Cross-validation score")

plt.fill_between(train_sizes1, train_scores_mean1 - train_scores_std1, train_scores_mean1 + train_scores_std1, alpha=0.1, color="r")
plt.fill_between(train_sizes1, test_scores_mean1 - test_scores_std1, test_scores_mean1 + test_scores_std1, alpha=0.1, color="g")

plt.legend()
plt.show()
```

```{python}
y_pred1 = clf1.predict(X_test1)
cm1 = confusion_matrix(y_test1, y_pred1, labels=clf1.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm1, display_labels=clf1.classes_)
disp.plot(cmap='Purples', values_format='d')
plt.show()
```

```{python}
scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
mean_score = scores.mean()
std_score = scores.std()
print(f'Standard Error Score: {std_score:.2f}')
```

```{python}
recall = recall_score(y_test, y_pred, average='macro')
print(f'Recall Score: {recall:.2f}')
```

```{python}
roc_auc = roc_auc_score(y_test, y_scores, multi_class='ovo')  
print(f'ROC AUC Score: {roc_auc:.2f}')
```

```{python}
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)

clf2 = SVC(probability=True, C=0.2, random_state=42)
clf2.fit(X_train2, y_train2)

y_scores2 = clf2.predict_proba(X_test2)

fpr2 = dict()
tpr2 = dict()
roc_auc2 = dict()

for i in range(len(clf2.classes_)):
    fpr2[i], tpr2[i], _ = roc_curve((y_test2 == clf2.classes_[i]).astype(int), y_scores2[:, i])
    roc_auc2[i] = auc(fpr2[i], tpr2[i])

plt.figure(figsize=(8, 6))

for i in range(len(clf2.classes_)):
    plt.plot(fpr2[i], tpr2[i], label=f'ROC curve (area = {roc_auc2[i]:.2f}) for class {clf2.classes_[i]}')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass Classification')
plt.legend(loc="lower right")
plt.show()
```
