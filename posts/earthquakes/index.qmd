---
title: "Earthquake Analysis"
author: "Mingyang Luo"
date: "2023-11-18"
categories: Non-linear Regression, Linear Regression
---

-   Simple regression model: The most fundamental regression model where predictions rely on a single feature from the dataset, also known as univariate regression.

    formula: $y = \beta_0 + \beta_1 x + \epsilon$

    Multiple regression model: In this type of regression model, predictions are derived from multiple features within the dataset, enabling a more comprehensive analysis of relationships between multiple variables.

    formula: $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \epsilon$

    In this post, the spotlight is on multiple regression.

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import learning_curve
from sklearn.multioutput import MultiOutputRegressor
from sklearn.svm import SVR
from sklearn.linear_model import Lasso
import warnings

warnings.filterwarnings("ignore")

csv_file_path = "earthquakes.csv"
df = pd.read_csv(csv_file_path)
df
```

```{python}
numeric = df.select_dtypes(include=['number'])
scaler = StandardScaler()
numeric1 = scaler.fit_transform(numeric)
numeric_df = pd.DataFrame(numeric1)
numeric_df.columns = numeric.columns
numeric_df.columns
```

The MultiOutputRegressor is employed to handle multiple outputs simultaneously. Within it, a pipeline is established with Polynomial Features (degree 2) and Lasso regularization, aiming to capture complex relationships and reduce overfitting.

The model's predictions on the test set are evaluated using Mean Squared Error (MSE), providing an assessment of how well the model performs in estimating the specified outputs.

```{python}
X = numeric_df[['location.depth', 'location.latitude','location.longitude', 'time.epoch', 'impact.gap']]
y = numeric_df[['impact.magnitude', 'time.month', 'time.day']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

poly_model = MultiOutputRegressor(make_pipeline(PolynomialFeatures(2, include_bias=False), Lasso(alpha=0.4)))
poly_model.fit(X_train, y_train)

poly_predictions = poly_model.predict(X_test)
mse = mean_squared_error(y_test, poly_predictions)
print(f"Mean Squared Error: {mse}")
```

Generate a learning curve to assesse model performance concerning varying training set sizes. The x-axis represents the number of training examples, while the y-axis indicates the Mean Squared Error (MSE) -- a measure of prediction accuracy.

The curve showcases how the model's error changes as the training set size increases. The 'Training error' line represents MSE on the training set, while 'Cross-validation error' signifies the model's performance on unseen data (validation set).

This visualization illuminates how the model's accuracy evolves with an increasing amount of training data. A narrowing gap between the training and cross-validation errors as the training set size grows indicates the model's ability to generalize to new data. Such insights help identify potential underfitting or overfitting issues, guiding adjustments to enhance the model's predictive capability.

```{python}
train_sizes, train_scores, test_scores = learning_curve(poly_model, X, y, cv=5, scoring='neg_mean_squared_error')

train_scores_mean = -train_scores.mean(axis=1)
test_scores_mean = -test_scores.mean(axis=1)

plt.figure()
plt.title("Learning Curve")
plt.xlabel("Training examples")
plt.ylabel("MSE")

plt.plot(train_sizes, train_scores_mean, label="Training error")
plt.plot(train_sizes, test_scores_mean, label="Cross-validation error")
plt.legend()
plt.show()
```

This code generates scatter plots for each input feature against various output variables. For every input feature, it showcases the model's predicted values (labeled 'Predicted') alongside the actual values (labeled 'Actual') for 'impact.magnitude', 'time.month', and 'time.day'.

These visualizations serve as a comparative analysis, allowing us to observe how well the model's predictions align with the actual values across different output variables concerning specific input features. Such assessments offer a nuanced understanding of the model's performance for various target variables in relation to specific input characteristics.

Each plot's alignment or deviation between predicted and actual values provides insights into the model's effectiveness in capturing relationships between input features and multiple output variables. It helps identify areas where the model excels or requires improvements.

```{python}
input_features = ['location.depth', 'location.latitude', 'location.longitude', 'time.epoch', 'impact.gap']
output_variables = ['impact.magnitude', 'time.month', 'time.day']

for input_feat in input_features:
    plt.figure(figsize=(8, 6))

    for output_var in output_variables:
        plt.scatter(X_test[input_feat], y_test[output_var], label=f'Actual {output_var}', alpha=0.6)
        plt.scatter(X_test[input_feat], poly_predictions[:, output_variables.index(output_var)], label=f'Predicted {output_var}', alpha=0.6)

        plt.xlabel(input_feat)
        plt.ylabel('Output Variables')
        plt.title(f'Output Variables vs {input_feat}')
        plt.legend()

    plt.show()
```

The scatter points depict the actual values of the output variable concerning the input feature, while the regression line showcases the model's predicted values across the input feature range. This visual representation offers a clear comparison between predicted and actual values, aiding in understanding how well the model captures the relationship between input features and multiple output variables.

```{python}
for input_feat in input_features:
    for output_var in output_variables:
        plt.figure(figsize=(8, 6))
        
        # Plot the actual values
        plt.scatter(X_test[input_feat], y_test[output_var], c='forestgreen', label=f'Actual {output_var}', alpha=0.6)
        
        # Plot the predicted values with a line
        sorted_indices = X_test[input_feat].argsort()
        plt.plot(X_test[input_feat].values[sorted_indices], poly_predictions[:, output_variables.index(output_var)][sorted_indices], 
                 label=f'Predicted {output_var}', color='chocolate', lw=1)

        plt.xlabel(input_feat)
        plt.ylabel('Output Variables')
        plt.title(f'Output Variables vs {input_feat}')
        plt.legend()

        plt.show()
```

```{python}
svm_model = MultiOutputRegressor(SVR(kernel='rbf', C=0.1))

svm_model.fit(X_train, y_train)

svm_predictions = svm_model.predict(X_test)
mse = mean_squared_error(y_test, svm_predictions)
print(f"Mean Squared Error with SVM: {mse}")
```

```{python}
train_sizes1, train_scores1, test_scores1 = learning_curve(svm_model, X, y, cv=3, scoring='neg_mean_squared_error')

train_scores_mean1 = -train_scores1.mean(axis=1)
test_scores_mean1 = -test_scores1.mean(axis=1)

plt.figure()
plt.title("Learning Curve")
plt.xlabel("Training examples")
plt.ylabel("MSE")

plt.plot(train_sizes1, train_scores_mean1, label="Training error")
plt.plot(train_sizes1, test_scores_mean1, label="Cross-validation error")
plt.legend()
plt.show()
```

```{python}
for input_feat in input_features:
    plt.figure(figsize=(8, 6))
    
    for output_var in output_variables:
        # Get the actual and predicted values for the specific output variable
        actual_values = y_test[output_var]
        predicted_values = svm_predictions[:, output_variables.index(output_var)]
        
        plt.scatter(X_test[input_feat], actual_values, label=f'Actual {output_var}', alpha=0.6)
        plt.scatter(X_test[input_feat], predicted_values, label=f'Predicted {output_var}', alpha=0.6)
    
    plt.xlabel(input_feat)
    plt.ylabel('Output Variables')
    plt.title(f'Output Variables vs {input_feat}')
    plt.legend()
    plt.show()
```

```{python}
for input_feat in input_features:
    for output_var in output_variables:
        plt.figure(figsize=(8, 6))
        
        # Plot the actual values
        plt.scatter(X_test[input_feat], y_test[output_var], color='palevioletred', label=f'Actual {output_var}', alpha=0.6)
        
        # Plot the predicted values with a line
        sorted_indices = X_test[input_feat].argsort()
        plt.plot(X_test[input_feat].values[sorted_indices], svm_predictions[:, output_variables.index(output_var)][sorted_indices], 
                 label=f'Predicted {output_var}', color='darkorchid', lw=1)

        plt.xlabel(input_feat)
        plt.ylabel('Output Variables')
        plt.title(f'Output Variables vs {input_feat}')
        plt.legend()

        plt.show()
```
