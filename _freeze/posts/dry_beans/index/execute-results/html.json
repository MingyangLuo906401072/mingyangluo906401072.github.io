{
  "hash": "8e0967ce043a17168dae10ce7a75e5ba",
  "result": {
    "markdown": "---\ntitle: \"Dry Beans Anaysis\"\nauthor: \"Mingyang Luo\"\ndate: \"2023-11-08\"\ncategories: [Classification, Clustering, Anomaly Detection]\nimage: \"image.jpg\"\n---\n\n**`DecisionTreeClassifier`** operates by partitioning the dataset into subsets based on the values of input features. It creates a tree-like structure where each node represents a feature, each branch denotes a decision based on that feature, and each leaf node signifies the class label or outcome. While decision trees are interpretable and intuitive, they are prone to overfitting when the tree grows too complex.\n\nOn the other hand, **`RandomForestClassifier`** is an ensemble learning method that builds multiple decision trees and amalgamates their predictions to make a more robust and accurate classification. It operates by constructing a multitude of decision trees, each trained on a random subset of the dataset and using a random subset of features. The final prediction is determined by aggregating the predictions of all the individual trees, usually through voting or averaging. This ensemble approach tends to reduce overfitting and enhances the model's generalizability.\n\nThe differences lie in their methodologies and performances. Decision trees are simple to interpret but might overfit the training data. Random forests, leveraging the wisdom of multiple trees, tend to offer higher accuracy and better resilience against overfitting. However, they might be less interpretable due to the complexity of aggregating predictions from multiple trees.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, roc_auc_score\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import label_binarize\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nexcel_file_path = \"DryBeanDataset/Dry_Bean_Dataset.xlsx\"\ndf = pd.read_excel(excel_file_path)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Perimeter</th>\n      <th>MajorAxisLength</th>\n      <th>MinorAxisLength</th>\n      <th>AspectRation</th>\n      <th>Eccentricity</th>\n      <th>ConvexArea</th>\n      <th>EquivDiameter</th>\n      <th>Extent</th>\n      <th>Solidity</th>\n      <th>roundness</th>\n      <th>Compactness</th>\n      <th>ShapeFactor1</th>\n      <th>ShapeFactor2</th>\n      <th>ShapeFactor3</th>\n      <th>ShapeFactor4</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28395</td>\n      <td>610.291</td>\n      <td>208.178117</td>\n      <td>173.888747</td>\n      <td>1.197191</td>\n      <td>0.549812</td>\n      <td>28715</td>\n      <td>190.141097</td>\n      <td>0.763923</td>\n      <td>0.988856</td>\n      <td>0.958027</td>\n      <td>0.913358</td>\n      <td>0.007332</td>\n      <td>0.003147</td>\n      <td>0.834222</td>\n      <td>0.998724</td>\n      <td>SEKER</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>28734</td>\n      <td>638.018</td>\n      <td>200.524796</td>\n      <td>182.734419</td>\n      <td>1.097356</td>\n      <td>0.411785</td>\n      <td>29172</td>\n      <td>191.272750</td>\n      <td>0.783968</td>\n      <td>0.984986</td>\n      <td>0.887034</td>\n      <td>0.953861</td>\n      <td>0.006979</td>\n      <td>0.003564</td>\n      <td>0.909851</td>\n      <td>0.998430</td>\n      <td>SEKER</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29380</td>\n      <td>624.110</td>\n      <td>212.826130</td>\n      <td>175.931143</td>\n      <td>1.209713</td>\n      <td>0.562727</td>\n      <td>29690</td>\n      <td>193.410904</td>\n      <td>0.778113</td>\n      <td>0.989559</td>\n      <td>0.947849</td>\n      <td>0.908774</td>\n      <td>0.007244</td>\n      <td>0.003048</td>\n      <td>0.825871</td>\n      <td>0.999066</td>\n      <td>SEKER</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30008</td>\n      <td>645.884</td>\n      <td>210.557999</td>\n      <td>182.516516</td>\n      <td>1.153638</td>\n      <td>0.498616</td>\n      <td>30724</td>\n      <td>195.467062</td>\n      <td>0.782681</td>\n      <td>0.976696</td>\n      <td>0.903936</td>\n      <td>0.928329</td>\n      <td>0.007017</td>\n      <td>0.003215</td>\n      <td>0.861794</td>\n      <td>0.994199</td>\n      <td>SEKER</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30140</td>\n      <td>620.134</td>\n      <td>201.847882</td>\n      <td>190.279279</td>\n      <td>1.060798</td>\n      <td>0.333680</td>\n      <td>30417</td>\n      <td>195.896503</td>\n      <td>0.773098</td>\n      <td>0.990893</td>\n      <td>0.984877</td>\n      <td>0.970516</td>\n      <td>0.006697</td>\n      <td>0.003665</td>\n      <td>0.941900</td>\n      <td>0.999166</td>\n      <td>SEKER</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13606</th>\n      <td>42097</td>\n      <td>759.696</td>\n      <td>288.721612</td>\n      <td>185.944705</td>\n      <td>1.552728</td>\n      <td>0.765002</td>\n      <td>42508</td>\n      <td>231.515799</td>\n      <td>0.714574</td>\n      <td>0.990331</td>\n      <td>0.916603</td>\n      <td>0.801865</td>\n      <td>0.006858</td>\n      <td>0.001749</td>\n      <td>0.642988</td>\n      <td>0.998385</td>\n      <td>DERMASON</td>\n    </tr>\n    <tr>\n      <th>13607</th>\n      <td>42101</td>\n      <td>757.499</td>\n      <td>281.576392</td>\n      <td>190.713136</td>\n      <td>1.476439</td>\n      <td>0.735702</td>\n      <td>42494</td>\n      <td>231.526798</td>\n      <td>0.799943</td>\n      <td>0.990752</td>\n      <td>0.922015</td>\n      <td>0.822252</td>\n      <td>0.006688</td>\n      <td>0.001886</td>\n      <td>0.676099</td>\n      <td>0.998219</td>\n      <td>DERMASON</td>\n    </tr>\n    <tr>\n      <th>13608</th>\n      <td>42139</td>\n      <td>759.321</td>\n      <td>281.539928</td>\n      <td>191.187979</td>\n      <td>1.472582</td>\n      <td>0.734065</td>\n      <td>42569</td>\n      <td>231.631261</td>\n      <td>0.729932</td>\n      <td>0.989899</td>\n      <td>0.918424</td>\n      <td>0.822730</td>\n      <td>0.006681</td>\n      <td>0.001888</td>\n      <td>0.676884</td>\n      <td>0.996767</td>\n      <td>DERMASON</td>\n    </tr>\n    <tr>\n      <th>13609</th>\n      <td>42147</td>\n      <td>763.779</td>\n      <td>283.382636</td>\n      <td>190.275731</td>\n      <td>1.489326</td>\n      <td>0.741055</td>\n      <td>42667</td>\n      <td>231.653248</td>\n      <td>0.705389</td>\n      <td>0.987813</td>\n      <td>0.907906</td>\n      <td>0.817457</td>\n      <td>0.006724</td>\n      <td>0.001852</td>\n      <td>0.668237</td>\n      <td>0.995222</td>\n      <td>DERMASON</td>\n    </tr>\n    <tr>\n      <th>13610</th>\n      <td>42159</td>\n      <td>772.237</td>\n      <td>295.142741</td>\n      <td>182.204716</td>\n      <td>1.619841</td>\n      <td>0.786693</td>\n      <td>42600</td>\n      <td>231.686223</td>\n      <td>0.788962</td>\n      <td>0.989648</td>\n      <td>0.888380</td>\n      <td>0.784997</td>\n      <td>0.007001</td>\n      <td>0.001640</td>\n      <td>0.616221</td>\n      <td>0.998180</td>\n      <td>DERMASON</td>\n    </tr>\n  </tbody>\n</table>\n<p>13611 rows Ã— 17 columns</p>\n</div>\n```\n:::\n:::\n\n\nParallel coordinate plots, a visualization method, display how different categories behave across various standardized numeric dimensions, offering a clear visual narrative of patterns and trends within the dataset. Together, these tools provide a powerful lens to explore and understand relationships among normalized numeric features.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nnumeric_df = df.select_dtypes(include=['number'])\ncols = numeric_df.columns\nscaler = StandardScaler()\nnumeric_df = scaler.fit_transform(numeric_df)\npca = PCA(n_components=2)\ndf_pca = pca.fit_transform(numeric_df)\nnormalized_df = pd.DataFrame(numeric_df, columns=cols)\n\nplt.figure(figsize=(23, 6))  \npd.plotting.parallel_coordinates(normalized_df.join(df.Class), class_column='Class', colormap='hsv') \nplt.title('Standardized Parallel Coordinates Plot')\nplt.ylabel('Values')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1832 height=505}\n:::\n:::\n\n\nUse KMeans clustering for visualization of how the dry bean dataset organizes into distinct clusters. The identified anomalies, highlighted in the plot, serve as potential outliers worth examining further. This methodology aids in both understanding the dataset's natural groupings and pinpointing potential irregularities.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nkmeans = KMeans(n_clusters=7, random_state=42)\ndf['Cluster'] = kmeans.fit_predict(df_pca)\nlabels = kmeans.labels_\n\ncluster_centers = kmeans.cluster_centers_\n\ndistances = []\nfor i, label in enumerate(kmeans.labels_):\n    center = cluster_centers[label]\n    point = df_pca[i]\n    distance = np.linalg.norm(point - center)  # Euclidean distance\n    distances.append(distance)\n\nthreshold = np.percentile(distances, 99)  \nanomalies = df_pca[np.array(distances) > threshold]\n\nplt.figure(figsize=(8, 6))\nplt.scatter(df_pca[:, 0], df_pca[:, 1], c=labels, cmap='viridis', marker='o', alpha=0.6)\nplt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], s=100, c='cyan', marker='o', label='Cluster Centers')\nplt.xlabel(f'Principal Component 1 (Explained Variance: {pca.explained_variance_ratio_[0]:.3f}')\nplt.ylabel(f'Principal Component 2 (Explained Variance: {pca.explained_variance_ratio_[1]:.3f}')\nplt.title('Clustering of Dry Beans using KMeans with 7 clusters')\nplt.scatter(anomalies[:, 0], anomalies[:, 1], c='red', marker='x', s=100, label='Anomalies')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=662 height=523}\n:::\n:::\n\n\nVisualize a Decision Tree Classifier's decision boundary in a PCA-transformed dataset. By creating a meshgrid and predicting class labels across this grid, the plot illustrates how the classifier categorizes data in the transformed feature space.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nX1 = df_pca\ny1 = df['Class']\nlabel_encoder = LabelEncoder()\ny1 = label_encoder.fit_transform(y1)\nfeature_1, feature_2 = np.meshgrid(\n    np.linspace(X1[:, 0].min(), X1[:, 0].max()),\n    np.linspace(X1[:, 1].min(), X1[:, 1].max())\n)\ngrid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\ntree = DecisionTreeClassifier().fit(X1, y1)\ny_pred1 = tree.predict(grid)\ny_pred1 = y_pred1.reshape(feature_1.shape)\ndisplay = DecisionBoundaryDisplay(xx0=feature_1, xx1=feature_2, response=y_pred1)\ndisplay.plot()\nplt.scatter(X1[:, 0], X1[:, 1], c=y1, edgecolor=\"k\", s=10)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=569 height=411}\n:::\n:::\n\n\nGenerates a summarized view of the distribution of classes within each cluster. The resulting table showcases the count of each class within every cluster, describing how classes are distributed across different clusters in the dataset.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncluster_data = pd.DataFrame({'Cluster': df['Cluster'], 'Class': df['Class']})  \ncluster_summary = cluster_data.groupby('Cluster')['Class'].value_counts().unstack().fillna(0)\nprint(cluster_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClass    BARBUNYA  BOMBAY    CALI  DERMASON   HOROZ   SEKER    SIRA\nCluster                                                            \n0             9.0     0.0     2.0     116.0     0.0  1869.0    29.0\n1            12.0     0.0    23.0      12.0  1814.0     1.0    72.0\n2            98.0     0.0    13.0     457.0    52.0    77.0  2280.0\n3             1.0   520.0     0.0       0.0     0.0     0.0     0.0\n4             0.0     0.0     0.0    2961.0     3.0    77.0   249.0\n5           824.0     2.0   347.0       0.0     0.0     3.0     1.0\n6           378.0     0.0  1245.0       0.0    59.0     0.0     5.0\n```\n:::\n:::\n\n\nGenerates a confusion matrix (cm) comparing predicted labels y_pred against the actual labels y_test to evaluate model performance. The ConfusionMatrixDisplay from scikit-learn visualizes this matrix, providing insights into the classifier's accuracy in predicting different classes. The color-coded matrix, displayed using a blue colormap, demonstrates the classifier's performance across different classes, enabling an assessment of its predictive strengths and weaknesses.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nX = numeric_df \ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = DecisionTreeClassifier(max_depth=8, random_state=42)\nclf.fit(X_train, y_train)\n\ny_scores = clf.predict_proba(X_test)\ny_pred = clf.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\ndisp.plot(cmap='Blues', values_format='d')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=573 height=431}\n:::\n:::\n\n\nThe resulting plot visualizes the ROC curves for each class, with the x-axis representing the False Positive Rate and the y-axis representing the True Positive Rate. The dashed diagonal line represents a random classifier. The curves' proximity to the upper-left corner indicates better classification performance, and the AUC values quantify the overall predictive power for each class.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ny_bin = label_binarize(y_test, classes=clf.classes_)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nfor i in range(len(clf.classes_)):\n    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_scores[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Plot ROC curves\nplt.figure(figsize=(8, 6))\n\nfor i in range(len(clf.classes_)):\n    plt.plot(fpr[i], tpr[i], label=f'ROC curve (area = {roc_auc[i]:.2f}) for class {clf.classes_[i]}')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass Classification')\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=674 height=523}\n:::\n:::\n\n\nThe shaded areas around each curve depict the variance (standard deviation) of the scores across different cross-validation folds for both training and testing data. This learning curve explains the model's behavior related to dataset sizes.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntrain_sizes, train_scores, test_scores = learning_curve(clf, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring='accuracy')\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.figure()\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training examples\")\nplt.ylabel(\"Accuracy\")\n\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=608 height=449}\n:::\n:::\n\n\nThis visualization serves as a visual roadmap of the Decision Tree Classifier's decision-making process. It encapsulates how the classifier discerns between different classes based on distinct features. The parameter max_depth=4 limits the depth of the tree, enhancing its readability and interpretability.\n\nAn interesting observation emerges from this visualization: the 'BOMBAY' class is entirely classified within the 4th depth of this decision tree. This finding aligns seamlessly with the ROC curve analysis, indicating a perfect 100 percent true positive rate for 'BOMBAY'. This convergence between the decision tree's structure and the ROC curve reinforces the accuracy of the classifier in precisely identifying instances belonging to the 'BOMBAY' class.\n\nThe correlation showcases model's capability to accurately classify instances of the 'BOMBAY' class, which could well explain previous and subsequent parts' roc curve's behavior.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom IPython.display import Image\nimport pydot\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\ndata=export_graphviz(\n    clf,\n    feature_names=df.drop(columns=['Class', 'Cluster']).columns,\n    class_names=clf.classes_,\n    rounded=True,\n    filled=True,\n    max_depth=4,\n)\n\ngraph = pydot.graph_from_dot_data(data)\nImage(graph[0].create_png())\n\n# graph = graphviz.Source(data)\n# graph.render(\"decision_tree\")\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nroc_auc = roc_auc_score(y_test, y_scores, multi_class='ovo')  \nprint(f'ROC AUC Score: {roc_auc:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nROC AUC Score: 0.98\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nscores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\nmean_score = scores.mean()\nstd_score = scores.std()\nprint(f'Standard Error Score: {std_score:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStandard Error Score: 0.18\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# recall = recall_score(y_test, y_pred, average=None)\nrecall = recall_score(y_test, y_pred, average='macro')\nprint(f'Recall Score: {recall:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall Score: 0.91\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf1 = RandomForestClassifier(n_estimators=20, max_depth=8, random_state=42)\nclf1.fit(X_train1, y_train1)\n\ny_scores1 = clf1.predict_proba(X_test1)\n\nfpr1 = dict()\ntpr1 = dict()\nroc_auc1 = dict()\n\nfor i in range(len(clf1.classes_)):\n    fpr1[i], tpr1[i], _ = roc_curve((y_test1 == clf1.classes_[i]).astype(int), y_scores1[:, i])\n    roc_auc1[i] = auc(fpr1[i], tpr1[i])\n\nplt.figure(figsize=(8, 6))\n\nfor i in range(len(clf1.classes_)):\n    plt.plot(fpr1[i], tpr1[i], label=f'ROC curve (area = {roc_auc1[i]:.2f}) for class {clf1.classes_[i]}')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass Classification')\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=674 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ntrain_sizes1, train_scores1, test_scores1 = learning_curve(clf1, X_train1, y_train1, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring='accuracy')\n\ntrain_scores_mean1 = np.mean(train_scores1, axis=1)\ntrain_scores_std1 = np.std(train_scores1, axis=1)\ntest_scores_mean1 = np.mean(test_scores1, axis=1)\ntest_scores_std1 = np.std(test_scores1, axis=1)\n\nplt.figure()\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training examples\")\nplt.ylabel(\"Accuracy\")\n\nplt.plot(train_sizes1, train_scores_mean1, 'o-', color=\"r\", label=\"Training score\")\nplt.plot(train_sizes1, test_scores_mean1, 'o-', color=\"g\", label=\"Cross-validation score\")\n\nplt.fill_between(train_sizes1, train_scores_mean1 - train_scores_std1, train_scores_mean1 + train_scores_std1, alpha=0.1, color=\"r\")\nplt.fill_between(train_sizes1, test_scores_mean1 - test_scores_std1, test_scores_mean1 + test_scores_std1, alpha=0.1, color=\"g\")\n\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=608 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ny_pred1 = clf1.predict(X_test1)\ncm1 = confusion_matrix(y_test1, y_pred1, labels=clf1.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm1, display_labels=clf1.classes_)\ndisp.plot(cmap='Purples', values_format='d')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=573 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nscores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\nmean_score = scores.mean()\nstd_score = scores.std()\nprint(f'Standard Error Score: {std_score:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStandard Error Score: 0.18\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nrecall = recall_score(y_test, y_pred, average='macro')\nprint(f'Recall Score: {recall:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecall Score: 0.91\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nroc_auc = roc_auc_score(y_test, y_scores, multi_class='ovo')  \nprint(f'ROC AUC Score: {roc_auc:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nROC AUC Score: 0.98\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf2 = SVC(probability=True, C=0.2, random_state=42)\nclf2.fit(X_train2, y_train2)\n\ny_scores2 = clf2.predict_proba(X_test2)\n\nfpr2 = dict()\ntpr2 = dict()\nroc_auc2 = dict()\n\nfor i in range(len(clf2.classes_)):\n    fpr2[i], tpr2[i], _ = roc_curve((y_test2 == clf2.classes_[i]).astype(int), y_scores2[:, i])\n    roc_auc2[i] = auc(fpr2[i], tpr2[i])\n\nplt.figure(figsize=(8, 6))\n\nfor i in range(len(clf2.classes_)):\n    plt.plot(fpr2[i], tpr2[i], label=f'ROC curve (area = {roc_auc2[i]:.2f}) for class {clf2.classes_[i]}')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass Classification')\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=674 height=523}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}